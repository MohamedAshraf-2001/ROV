{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data in Code \n",
    "\n",
    "Data processing for deep learning will vary greatly depending on the type of data we're working with and the type of task we'll be using the network for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from random import randint \n",
    "from sklearn.utils import shuffle\n",
    "# There is another way of data scaling, where the minimum of feature\n",
    "# is made equal to zero and the maximum of feature equal to one.\n",
    "# MinMax Scaler shrinks the data within the given range, \n",
    "# usually of 0 to 1. It transforms data by scaling features to a \n",
    "# given range. It scales the values to a specific value range \n",
    "# without changing the shape of the original distribution.\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [] #target data\n",
    "train_samples = [] #input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Creation\n",
    "For this simple task, we'll be creating our own example data set.\n",
    "\n",
    "As motivation for this data, let's suppose that an experimental drug was tested on individuals ranging from age 13 to 100 in a clinical trial. The trial had 2100 participants. Half of the participants were under 65 years old, and the other half was 65 years of age or older.\n",
    "\n",
    "The trial showed that around 95% of patients 65 or older experienced side effects from the drug, and around 95% of patients under 65 experienced no side effects, generally showing that elderly individuals were more likely to experience side effects.\n",
    "\n",
    "Ultimately, we want to build a model to tell us whether or not a patient will experience side effects solely based on the patient's age. The judgement of the model will be based on the training data.\n",
    "\n",
    "Note that with the simplicity of the data along with the conclusions drawn from it, a neural network may be overkill, but understand this is just to first get introduced to working with data for deep learning, and later, we'll be making use of more advanced data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels 1 ==> did experience, 0 ==> didn't experience:\n",
    "\n",
    "for i in range(50):\n",
    "    #The ~5% of younger individuals who did experience side effects.\n",
    "    random_younger = randint(13,64)\n",
    "    train_samples.append(random_younger)\n",
    "    train_labels.append(1)\n",
    "\n",
    "    #The ~5% of older individuals who didn't experience side effects.\n",
    "    random_older = randint(65,100)\n",
    "    train_samples.append(random_older)\n",
    "    train_labels.append(0)\n",
    "\n",
    "for i in range(1000):\n",
    "    # The ~95% of younger individuals who didn't experience side effects.\n",
    "    random_younger = randint(13,64)\n",
    "    train_samples.append(random_younger)\n",
    "    train_labels.append(0)\n",
    "\n",
    "    #The ~95% of older individuals who did experience side effects.\n",
    "    random_older = randint(65,100)\n",
    "    train_samples.append(random_older)\n",
    "    train_labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[49,\n",
       " 70,\n",
       " 34,\n",
       " 93,\n",
       " 40,\n",
       " 69,\n",
       " 53,\n",
       " 96,\n",
       " 19,\n",
       " 91,\n",
       " 18,\n",
       " 77,\n",
       " 15,\n",
       " 67,\n",
       " 34,\n",
       " 85,\n",
       " 28,\n",
       " 81,\n",
       " 40,\n",
       " 67,\n",
       " 62,\n",
       " 76,\n",
       " 45,\n",
       " 87,\n",
       " 44,\n",
       " 91,\n",
       " 16,\n",
       " 80,\n",
       " 35,\n",
       " 73,\n",
       " 58,\n",
       " 83,\n",
       " 44,\n",
       " 89,\n",
       " 14,\n",
       " 73,\n",
       " 30,\n",
       " 84,\n",
       " 56,\n",
       " 83,\n",
       " 48,\n",
       " 78,\n",
       " 24,\n",
       " 73,\n",
       " 15,\n",
       " 99,\n",
       " 24,\n",
       " 77,\n",
       " 19,\n",
       " 86,\n",
       " 64,\n",
       " 74,\n",
       " 15,\n",
       " 79,\n",
       " 18,\n",
       " 77,\n",
       " 36,\n",
       " 95,\n",
       " 44,\n",
       " 87,\n",
       " 37,\n",
       " 93,\n",
       " 46,\n",
       " 96,\n",
       " 22,\n",
       " 98,\n",
       " 42,\n",
       " 97,\n",
       " 60,\n",
       " 93,\n",
       " 28,\n",
       " 65,\n",
       " 30,\n",
       " 92,\n",
       " 42,\n",
       " 82,\n",
       " 14,\n",
       " 92,\n",
       " 14,\n",
       " 83,\n",
       " 17,\n",
       " 85,\n",
       " 56,\n",
       " 100,\n",
       " 29,\n",
       " 79,\n",
       " 27,\n",
       " 99,\n",
       " 17,\n",
       " 75,\n",
       " 13,\n",
       " 66,\n",
       " 39,\n",
       " 87,\n",
       " 13,\n",
       " 99,\n",
       " 15,\n",
       " 77,\n",
       " 28,\n",
       " 72,\n",
       " 62,\n",
       " 91,\n",
       " 16,\n",
       " 88,\n",
       " 31,\n",
       " 82,\n",
       " 19,\n",
       " 71,\n",
       " 18,\n",
       " 79,\n",
       " 27,\n",
       " 84,\n",
       " 51,\n",
       " 71,\n",
       " 17,\n",
       " 78,\n",
       " 36,\n",
       " 77,\n",
       " 51,\n",
       " 69,\n",
       " 42,\n",
       " 90,\n",
       " 33,\n",
       " 72,\n",
       " 30,\n",
       " 93,\n",
       " 51,\n",
       " 89,\n",
       " 58,\n",
       " 69,\n",
       " 16,\n",
       " 69,\n",
       " 31,\n",
       " 96,\n",
       " 30,\n",
       " 75,\n",
       " 51,\n",
       " 68,\n",
       " 49,\n",
       " 67,\n",
       " 32,\n",
       " 68,\n",
       " 16,\n",
       " 67,\n",
       " 42,\n",
       " 100,\n",
       " 38,\n",
       " 80,\n",
       " 46,\n",
       " 92,\n",
       " 51,\n",
       " 88,\n",
       " 16,\n",
       " 74,\n",
       " 44,\n",
       " 75,\n",
       " 55,\n",
       " 70,\n",
       " 31,\n",
       " 70,\n",
       " 21,\n",
       " 66,\n",
       " 44,\n",
       " 72,\n",
       " 18,\n",
       " 99,\n",
       " 55,\n",
       " 74,\n",
       " 18,\n",
       " 82,\n",
       " 57,\n",
       " 86,\n",
       " 49,\n",
       " 87,\n",
       " 15,\n",
       " 81,\n",
       " 35,\n",
       " 82,\n",
       " 21,\n",
       " 67,\n",
       " 31,\n",
       " 75,\n",
       " 57,\n",
       " 67,\n",
       " 19,\n",
       " 81,\n",
       " 29,\n",
       " 68,\n",
       " 52,\n",
       " 76,\n",
       " 24,\n",
       " 93,\n",
       " 28,\n",
       " 70,\n",
       " 23,\n",
       " 82,\n",
       " 51,\n",
       " 99,\n",
       " 20,\n",
       " 93,\n",
       " 33,\n",
       " 91,\n",
       " 54,\n",
       " 76,\n",
       " 49,\n",
       " 68,\n",
       " 49,\n",
       " 99,\n",
       " 31,\n",
       " 68,\n",
       " 64,\n",
       " 75,\n",
       " 41,\n",
       " 91,\n",
       " 21,\n",
       " 89,\n",
       " 25,\n",
       " 71,\n",
       " 38,\n",
       " 89,\n",
       " 52,\n",
       " 74,\n",
       " 31,\n",
       " 81,\n",
       " 50,\n",
       " 94,\n",
       " 26,\n",
       " 96,\n",
       " 40,\n",
       " 73,\n",
       " 62,\n",
       " 69,\n",
       " 28,\n",
       " 75,\n",
       " 39,\n",
       " 69,\n",
       " 32,\n",
       " 80,\n",
       " 62,\n",
       " 88,\n",
       " 63,\n",
       " 97,\n",
       " 42,\n",
       " 67,\n",
       " 34,\n",
       " 68,\n",
       " 21,\n",
       " 79,\n",
       " 50,\n",
       " 100,\n",
       " 38,\n",
       " 92,\n",
       " 15,\n",
       " 99,\n",
       " 54,\n",
       " 92,\n",
       " 16,\n",
       " 93,\n",
       " 15,\n",
       " 90,\n",
       " 44,\n",
       " 74,\n",
       " 17,\n",
       " 69,\n",
       " 56,\n",
       " 88,\n",
       " 49,\n",
       " 77,\n",
       " 31,\n",
       " 66,\n",
       " 57,\n",
       " 78,\n",
       " 62,\n",
       " 68,\n",
       " 49,\n",
       " 75,\n",
       " 30,\n",
       " 94,\n",
       " 54,\n",
       " 79,\n",
       " 20,\n",
       " 67,\n",
       " 63,\n",
       " 92,\n",
       " 25,\n",
       " 90,\n",
       " 36,\n",
       " 89,\n",
       " 34,\n",
       " 88,\n",
       " 26,\n",
       " 83,\n",
       " 47,\n",
       " 94,\n",
       " 61,\n",
       " 96,\n",
       " 61,\n",
       " 88,\n",
       " 30,\n",
       " 78,\n",
       " 36,\n",
       " 100,\n",
       " 53,\n",
       " 76,\n",
       " 33,\n",
       " 66,\n",
       " 58,\n",
       " 66,\n",
       " 59,\n",
       " 99,\n",
       " 39,\n",
       " 73,\n",
       " 40,\n",
       " 82,\n",
       " 52,\n",
       " 88,\n",
       " 30,\n",
       " 99,\n",
       " 59,\n",
       " 81,\n",
       " 56,\n",
       " 98,\n",
       " 25,\n",
       " 82,\n",
       " 32,\n",
       " 97,\n",
       " 44,\n",
       " 92,\n",
       " 26,\n",
       " 74,\n",
       " 33,\n",
       " 100,\n",
       " 21,\n",
       " 72,\n",
       " 60,\n",
       " 75,\n",
       " 16,\n",
       " 84,\n",
       " 59,\n",
       " 81,\n",
       " 15,\n",
       " 97,\n",
       " 22,\n",
       " 68,\n",
       " 38,\n",
       " 98,\n",
       " 43,\n",
       " 97,\n",
       " 48,\n",
       " 71,\n",
       " 20,\n",
       " 74,\n",
       " 35,\n",
       " 100,\n",
       " 55,\n",
       " 83,\n",
       " 31,\n",
       " 69,\n",
       " 16,\n",
       " 79,\n",
       " 24,\n",
       " 68,\n",
       " 43,\n",
       " 92,\n",
       " 45,\n",
       " 84,\n",
       " 19,\n",
       " 97,\n",
       " 44,\n",
       " 80,\n",
       " 54,\n",
       " 97,\n",
       " 27,\n",
       " 94,\n",
       " 17,\n",
       " 100,\n",
       " 34,\n",
       " 76,\n",
       " 51,\n",
       " 84,\n",
       " 51,\n",
       " 83,\n",
       " 32,\n",
       " 92,\n",
       " 20,\n",
       " 71,\n",
       " 51,\n",
       " 75,\n",
       " 14,\n",
       " 74,\n",
       " 58,\n",
       " 92,\n",
       " 17,\n",
       " 81,\n",
       " 55,\n",
       " 92,\n",
       " 48,\n",
       " 74,\n",
       " 56,\n",
       " 79,\n",
       " 39,\n",
       " 87,\n",
       " 42,\n",
       " 74,\n",
       " 15,\n",
       " 90,\n",
       " 54,\n",
       " 97,\n",
       " 54,\n",
       " 81,\n",
       " 15,\n",
       " 72,\n",
       " 22,\n",
       " 82,\n",
       " 19,\n",
       " 96,\n",
       " 28,\n",
       " 93,\n",
       " 39,\n",
       " 72,\n",
       " 46,\n",
       " 67,\n",
       " 28,\n",
       " 82,\n",
       " 39,\n",
       " 68,\n",
       " 51,\n",
       " 77,\n",
       " 36,\n",
       " 80,\n",
       " 46,\n",
       " 91,\n",
       " 35,\n",
       " 89,\n",
       " 50,\n",
       " 90,\n",
       " 23,\n",
       " 81,\n",
       " 55,\n",
       " 88,\n",
       " 59,\n",
       " 85,\n",
       " 59,\n",
       " 85,\n",
       " 42,\n",
       " 77,\n",
       " 25,\n",
       " 68,\n",
       " 27,\n",
       " 92,\n",
       " 26,\n",
       " 70,\n",
       " 21,\n",
       " 92,\n",
       " 57,\n",
       " 67,\n",
       " 30,\n",
       " 97,\n",
       " 14,\n",
       " 71,\n",
       " 20,\n",
       " 92,\n",
       " 54,\n",
       " 100,\n",
       " 14,\n",
       " 89,\n",
       " 59,\n",
       " 99,\n",
       " 40,\n",
       " 91,\n",
       " 17,\n",
       " 81,\n",
       " 36,\n",
       " 85,\n",
       " 23,\n",
       " 75,\n",
       " 44,\n",
       " 72,\n",
       " 62,\n",
       " 96,\n",
       " 24,\n",
       " 87,\n",
       " 18,\n",
       " 88,\n",
       " 42,\n",
       " 99,\n",
       " 16,\n",
       " 66,\n",
       " 30,\n",
       " 76,\n",
       " 13,\n",
       " 77,\n",
       " 22,\n",
       " 79,\n",
       " 33,\n",
       " 93,\n",
       " 53,\n",
       " 95,\n",
       " 43,\n",
       " 96,\n",
       " 22,\n",
       " 89,\n",
       " 40,\n",
       " 100,\n",
       " 58,\n",
       " 92,\n",
       " 37,\n",
       " 65,\n",
       " 18,\n",
       " 82,\n",
       " 23,\n",
       " 91,\n",
       " 61,\n",
       " 66,\n",
       " 35,\n",
       " 89,\n",
       " 32,\n",
       " 79,\n",
       " 31,\n",
       " 95,\n",
       " 24,\n",
       " 99,\n",
       " 63,\n",
       " 77,\n",
       " 56,\n",
       " 91,\n",
       " 57,\n",
       " 67,\n",
       " 53,\n",
       " 86,\n",
       " 22,\n",
       " 98,\n",
       " 44,\n",
       " 74,\n",
       " 64,\n",
       " 76,\n",
       " 27,\n",
       " 84,\n",
       " 35,\n",
       " 95,\n",
       " 52,\n",
       " 65,\n",
       " 63,\n",
       " 75,\n",
       " 18,\n",
       " 93,\n",
       " 57,\n",
       " 75,\n",
       " 35,\n",
       " 88,\n",
       " 19,\n",
       " 98,\n",
       " 32,\n",
       " 83,\n",
       " 14,\n",
       " 65,\n",
       " 41,\n",
       " 67,\n",
       " 19,\n",
       " 65,\n",
       " 18,\n",
       " 95,\n",
       " 61,\n",
       " 79,\n",
       " 21,\n",
       " 84,\n",
       " 52,\n",
       " 65,\n",
       " 51,\n",
       " 70,\n",
       " 56,\n",
       " 87,\n",
       " 46,\n",
       " 71,\n",
       " 20,\n",
       " 90,\n",
       " 41,\n",
       " 72,\n",
       " 17,\n",
       " 74,\n",
       " 21,\n",
       " 82,\n",
       " 36,\n",
       " 83,\n",
       " 47,\n",
       " 97,\n",
       " 43,\n",
       " 65,\n",
       " 46,\n",
       " 70,\n",
       " 32,\n",
       " 98,\n",
       " 42,\n",
       " 87,\n",
       " 40,\n",
       " 84,\n",
       " 24,\n",
       " 65,\n",
       " 30,\n",
       " 99,\n",
       " 24,\n",
       " 81,\n",
       " 49,\n",
       " 89,\n",
       " 38,\n",
       " 96,\n",
       " 33,\n",
       " 93,\n",
       " 35,\n",
       " 95,\n",
       " 22,\n",
       " 91,\n",
       " 41,\n",
       " 69,\n",
       " 59,\n",
       " 67,\n",
       " 19,\n",
       " 65,\n",
       " 61,\n",
       " 74,\n",
       " 49,\n",
       " 93,\n",
       " 17,\n",
       " 65,\n",
       " 19,\n",
       " 89,\n",
       " 17,\n",
       " 94,\n",
       " 27,\n",
       " 87,\n",
       " 33,\n",
       " 73,\n",
       " 62,\n",
       " 96,\n",
       " 47,\n",
       " 72,\n",
       " 18,\n",
       " 84,\n",
       " 24,\n",
       " 88,\n",
       " 34,\n",
       " 78,\n",
       " 49,\n",
       " 84,\n",
       " 26,\n",
       " 77,\n",
       " 21,\n",
       " 84,\n",
       " 18,\n",
       " 91,\n",
       " 21,\n",
       " 80,\n",
       " 44,\n",
       " 89,\n",
       " 33,\n",
       " 75,\n",
       " 39,\n",
       " 75,\n",
       " 14,\n",
       " 74,\n",
       " 53,\n",
       " 76,\n",
       " 49,\n",
       " 97,\n",
       " 52,\n",
       " 70,\n",
       " 52,\n",
       " 95,\n",
       " 38,\n",
       " 69,\n",
       " 33,\n",
       " 80,\n",
       " 30,\n",
       " 77,\n",
       " 18,\n",
       " 88,\n",
       " 51,\n",
       " 82,\n",
       " 56,\n",
       " 73,\n",
       " 43,\n",
       " 100,\n",
       " 32,\n",
       " 68,\n",
       " 23,\n",
       " 96,\n",
       " 43,\n",
       " 68,\n",
       " 46,\n",
       " 82,\n",
       " 33,\n",
       " 93,\n",
       " 37,\n",
       " 68,\n",
       " 53,\n",
       " 70,\n",
       " 57,\n",
       " 70,\n",
       " 19,\n",
       " 70,\n",
       " 46,\n",
       " 92,\n",
       " 29,\n",
       " 71,\n",
       " 63,\n",
       " 99,\n",
       " 30,\n",
       " 97,\n",
       " 38,\n",
       " 92,\n",
       " 38,\n",
       " 89,\n",
       " 44,\n",
       " 88,\n",
       " 15,\n",
       " 68,\n",
       " 30,\n",
       " 86,\n",
       " 33,\n",
       " 71,\n",
       " 26,\n",
       " 85,\n",
       " 60,\n",
       " 65,\n",
       " 58,\n",
       " 97,\n",
       " 31,\n",
       " 73,\n",
       " 15,\n",
       " 85,\n",
       " 32,\n",
       " 93,\n",
       " 61,\n",
       " 68,\n",
       " 51,\n",
       " 67,\n",
       " 17,\n",
       " 98,\n",
       " 15,\n",
       " 81,\n",
       " 61,\n",
       " 86,\n",
       " 42,\n",
       " 65,\n",
       " 17,\n",
       " 79,\n",
       " 36,\n",
       " 72,\n",
       " 22,\n",
       " 88,\n",
       " 25,\n",
       " 79,\n",
       " 53,\n",
       " 65,\n",
       " 50,\n",
       " 98,\n",
       " 41,\n",
       " 83,\n",
       " 24,\n",
       " 66,\n",
       " 63,\n",
       " 74,\n",
       " 16,\n",
       " 84,\n",
       " 39,\n",
       " 94,\n",
       " 17,\n",
       " 94,\n",
       " 39,\n",
       " 88,\n",
       " 52,\n",
       " 71,\n",
       " 59,\n",
       " 90,\n",
       " 49,\n",
       " 66,\n",
       " 17,\n",
       " 99,\n",
       " 60,\n",
       " 68,\n",
       " 53,\n",
       " 67,\n",
       " 59,\n",
       " 81,\n",
       " 42,\n",
       " 74,\n",
       " 29,\n",
       " 87,\n",
       " 18,\n",
       " 79,\n",
       " 46,\n",
       " 77,\n",
       " 62,\n",
       " 67,\n",
       " 51,\n",
       " 97,\n",
       " 50,\n",
       " 86,\n",
       " 49,\n",
       " 77,\n",
       " 18,\n",
       " 72,\n",
       " 43,\n",
       " 89,\n",
       " 25,\n",
       " 75,\n",
       " 42,\n",
       " 72,\n",
       " 31,\n",
       " 90,\n",
       " 34,\n",
       " 90,\n",
       " 31,\n",
       " 77,\n",
       " 32,\n",
       " 90,\n",
       " 42,\n",
       " 74,\n",
       " 34,\n",
       " 84,\n",
       " 52,\n",
       " 90,\n",
       " 62,\n",
       " 70,\n",
       " 54,\n",
       " 90,\n",
       " 44,\n",
       " 91,\n",
       " 24,\n",
       " 96,\n",
       " 55,\n",
       " 78,\n",
       " 28,\n",
       " 82,\n",
       " 14,\n",
       " 79,\n",
       " 61,\n",
       " 89,\n",
       " 17,\n",
       " 69,\n",
       " 38,\n",
       " 81,\n",
       " 42,\n",
       " 72,\n",
       " 36,\n",
       " 88,\n",
       " 60,\n",
       " 88,\n",
       " 23,\n",
       " 81,\n",
       " 29,\n",
       " 98,\n",
       " 45,\n",
       " 72,\n",
       " 61,\n",
       " 93,\n",
       " 49,\n",
       " 96,\n",
       " 27,\n",
       " 99,\n",
       " 56,\n",
       " 100,\n",
       " 60,\n",
       " 83,\n",
       " 41,\n",
       " 75,\n",
       " 41,\n",
       " 76,\n",
       " 50,\n",
       " 85,\n",
       " 63,\n",
       " 87,\n",
       " 30,\n",
       " 67,\n",
       " 38,\n",
       " 88,\n",
       " 32,\n",
       " 85,\n",
       " 42,\n",
       " 69,\n",
       " 17,\n",
       " 76,\n",
       " 57,\n",
       " 69,\n",
       " 60,\n",
       " 98,\n",
       " 29,\n",
       " 69,\n",
       " 35,\n",
       " 93,\n",
       " 57,\n",
       " 94,\n",
       " 58,\n",
       " 80,\n",
       " 60,\n",
       " 69,\n",
       " 44,\n",
       " 95,\n",
       " 26,\n",
       " 80,\n",
       " 38,\n",
       " 70,\n",
       " 44,\n",
       " 79,\n",
       " 14,\n",
       " 84,\n",
       " 25,\n",
       " 78,\n",
       " 13,\n",
       " 80,\n",
       " 17,\n",
       " 65,\n",
       " 61,\n",
       " 78,\n",
       " 22,\n",
       " 76,\n",
       " 43,\n",
       " 66,\n",
       " 59,\n",
       " 74,\n",
       " 21,\n",
       " 94,\n",
       " 13,\n",
       " 98,\n",
       " 59,\n",
       " 66,\n",
       " 53,\n",
       " 79,\n",
       " 25,\n",
       " 68,\n",
       " 37,\n",
       " 85,\n",
       " 47,\n",
       " 69,\n",
       " 22,\n",
       " 73,\n",
       " 24,\n",
       " 79,\n",
       " 35,\n",
       " 77,\n",
       " 33,\n",
       " 78,\n",
       " 63,\n",
       " 69,\n",
       " 40,\n",
       " 83,\n",
       " 35,\n",
       " 87,\n",
       " 43,\n",
       " 72,\n",
       " 22,\n",
       " 91,\n",
       " 59,\n",
       " 76,\n",
       " 57,\n",
       " 98,\n",
       " 15,\n",
       " 91,\n",
       " 44,\n",
       " 70,\n",
       " 31,\n",
       " 93,\n",
       " 16,\n",
       " 81,\n",
       " 62,\n",
       " 98,\n",
       " 25,\n",
       " 89,\n",
       " 58,\n",
       " 85,\n",
       " 24,\n",
       " 77,\n",
       " 14,\n",
       " 85,\n",
       " 36,\n",
       " 100,\n",
       " 19,\n",
       " 85,\n",
       " 21,\n",
       " 77,\n",
       " 43,\n",
       " 91,\n",
       " 56,\n",
       " 79,\n",
       " 23,\n",
       " 66,\n",
       " 31,\n",
       " 74,\n",
       " 46,\n",
       " 67,\n",
       " 20,\n",
       " 85,\n",
       " 27,\n",
       " 81,\n",
       " 49,\n",
       " 82,\n",
       " 47,\n",
       " 95,\n",
       " 39,\n",
       " 75,\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "We now convert both lists into numpy arrays due to what we discussed the fit() function expects, and we then shuffle the arrays to remove any order that was imposed on the data during the creation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  1],\n",
       "       [70,  0],\n",
       "       [34,  1],\n",
       "       ...,\n",
       "       [80,  1],\n",
       "       [16,  0],\n",
       "       [78,  1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = np.array(train_labels)\n",
    "train_samples = np.array(train_samples)\n",
    "np.c_[train_samples.reshape(2100,1), train_labels.reshape(2100,1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_samples = shuffle(train_labels, train_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use scikit-learn's MinMaxScaler class to scale all of the data down from a scale ranging from 13 to 100 to be on a scale from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = MinMaxScaler(feature_range=(0,1))\n",
    "# We reshape the data as a technical requirement just since \n",
    "# the fit_transform() function doesn't accept 1D data by default.\n",
    "scaled_train_samples = scalar.fit_transform(train_samples.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, all of the data has been transformed to numbers between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05747126],\n",
       "       [0.33333333],\n",
       "       [0.73563218],\n",
       "       ...,\n",
       "       [0.81609195],\n",
       "       [0.11494253],\n",
       "       [0.02298851]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_train_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create An Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from tensorflow.keras.metrics import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build A Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model is an instance of a Sequential object. A tf.keras.Sequential\n",
    "#model is a linear stack of layers. It accepts a list, and each element\n",
    "#in the list should be a layer.\n",
    "model = Sequential([\n",
    "    Dense(units=16, input_shape=(1,), activation = 'relu'), #input_layer.\n",
    "    Dense(units=32, activation = 'relu'), #hidden_layer.\n",
    "    Dense(units=2, activation= 'softmax') #output_layer.\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed, we'll be training our network on the data that we generated and processed in the previous episode, and recall, this data is one-dimensional. The input_shape parameter expects a tuple of integers that matches the shape of the input data, so we correspondingly specify (1, ) as the input_shape of our one-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling The Model\n",
    "The first thing we need to do to get the model ready for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when we have only two classes, we could instead configure our output layer to have only one output, rather than two, and use *binary_crossentropy* as our loss, rather than *categorical_crossentropy*. Both options work equally well and achieve the exact same result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training The Model\n",
    "Now that the model is compiled, we can train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "210/210 - 1s - loss: 0.6502 - accuracy: 0.5600 - 527ms/epoch - 3ms/step\n",
      "Epoch 2/30\n",
      "210/210 - 0s - loss: 0.6254 - accuracy: 0.6371 - 172ms/epoch - 820us/step\n",
      "Epoch 3/30\n",
      "210/210 - 0s - loss: 0.6019 - accuracy: 0.6838 - 170ms/epoch - 807us/step\n",
      "Epoch 4/30\n",
      "210/210 - 0s - loss: 0.5784 - accuracy: 0.7290 - 176ms/epoch - 836us/step\n",
      "Epoch 5/30\n",
      "210/210 - 0s - loss: 0.5539 - accuracy: 0.7633 - 174ms/epoch - 826us/step\n",
      "Epoch 6/30\n",
      "210/210 - 0s - loss: 0.5289 - accuracy: 0.7881 - 179ms/epoch - 850us/step\n",
      "Epoch 7/30\n",
      "210/210 - 0s - loss: 0.5038 - accuracy: 0.8081 - 171ms/epoch - 812us/step\n",
      "Epoch 8/30\n",
      "210/210 - 0s - loss: 0.4787 - accuracy: 0.8310 - 174ms/epoch - 826us/step\n",
      "Epoch 9/30\n",
      "210/210 - 0s - loss: 0.4543 - accuracy: 0.8486 - 179ms/epoch - 850us/step\n",
      "Epoch 10/30\n",
      "210/210 - 0s - loss: 0.4313 - accuracy: 0.8619 - 177ms/epoch - 841us/step\n",
      "Epoch 11/30\n",
      "210/210 - 0s - loss: 0.4102 - accuracy: 0.8738 - 190ms/epoch - 907us/step\n",
      "Epoch 12/30\n",
      "210/210 - 0s - loss: 0.3912 - accuracy: 0.8838 - 223ms/epoch - 1ms/step\n",
      "Epoch 13/30\n",
      "210/210 - 0s - loss: 0.3740 - accuracy: 0.8890 - 210ms/epoch - 1ms/step\n",
      "Epoch 14/30\n",
      "210/210 - 0s - loss: 0.3590 - accuracy: 0.9005 - 207ms/epoch - 988us/step\n",
      "Epoch 15/30\n",
      "210/210 - 0s - loss: 0.3462 - accuracy: 0.9057 - 207ms/epoch - 988us/step\n",
      "Epoch 16/30\n",
      "210/210 - 0s - loss: 0.3351 - accuracy: 0.9095 - 188ms/epoch - 896us/step\n",
      "Epoch 17/30\n",
      "210/210 - 0s - loss: 0.3257 - accuracy: 0.9186 - 183ms/epoch - 869us/step\n",
      "Epoch 18/30\n",
      "210/210 - 0s - loss: 0.3176 - accuracy: 0.9181 - 202ms/epoch - 964us/step\n",
      "Epoch 19/30\n",
      "210/210 - 0s - loss: 0.3108 - accuracy: 0.9229 - 183ms/epoch - 869us/step\n",
      "Epoch 20/30\n",
      "210/210 - 0s - loss: 0.3050 - accuracy: 0.9262 - 168ms/epoch - 798us/step\n",
      "Epoch 21/30\n",
      "210/210 - 0s - loss: 0.2999 - accuracy: 0.9271 - 162ms/epoch - 769us/step\n",
      "Epoch 22/30\n",
      "210/210 - 0s - loss: 0.2955 - accuracy: 0.9290 - 164ms/epoch - 779us/step\n",
      "Epoch 23/30\n",
      "210/210 - 0s - loss: 0.2916 - accuracy: 0.9338 - 184ms/epoch - 874us/step\n",
      "Epoch 24/30\n",
      "210/210 - 0s - loss: 0.2888 - accuracy: 0.9295 - 167ms/epoch - 793us/step\n",
      "Epoch 25/30\n",
      "210/210 - 0s - loss: 0.2860 - accuracy: 0.9319 - 160ms/epoch - 760us/step\n",
      "Epoch 26/30\n",
      "210/210 - 0s - loss: 0.2838 - accuracy: 0.9324 - 157ms/epoch - 746us/step\n",
      "Epoch 27/30\n",
      "210/210 - 0s - loss: 0.2814 - accuracy: 0.9357 - 156ms/epoch - 741us/step\n",
      "Epoch 28/30\n",
      "210/210 - 0s - loss: 0.2796 - accuracy: 0.9352 - 157ms/epoch - 746us/step\n",
      "Epoch 29/30\n",
      "210/210 - 0s - loss: 0.2781 - accuracy: 0.9367 - 159ms/epoch - 755us/step\n",
      "Epoch 30/30\n",
      "210/210 - 0s - loss: 0.2766 - accuracy: 0.9367 - 157ms/epoch - 746us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17316d9ce88>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=scaled_train_samples, y=train_labels, batch_size=10, epochs=30, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we specify *verbose=2*. This just specifies how much output to the console we want to see during each epoch of training. The verbosity levels range from 0 to 2, so we're getting the most verbose output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 91ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([15,20,63,80,27,62])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is A Validation Set?\n",
    "\n",
    "Before training begins, we can choose to remove a portion of the training set and place it in a validation set. Then, during training, the model will train only on the training set, and it will validate by evaluating the data in the validation set.\n",
    "\n",
    "Essentially, the model is learning the features of the data in the training set, taking what it's learned from this data, and then predicting on the validation set. During each epoch, we will see not only the loss and accuracy results for the training set, but also for the validation set.\n",
    "\n",
    "This allows us to see how well the model is generalizing on data it wasn't trained on because, recall, the validation data should not be part of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "189/189 - 0s - loss: 0.2634 - accuracy: 0.9413 - val_loss: 0.3820 - val_accuracy: 0.9000 - 315ms/epoch - 2ms/step\n",
      "Epoch 2/30\n",
      "189/189 - 0s - loss: 0.2620 - accuracy: 0.9434 - val_loss: 0.3836 - val_accuracy: 0.9000 - 175ms/epoch - 923us/step\n",
      "Epoch 3/30\n",
      "189/189 - 0s - loss: 0.2608 - accuracy: 0.9392 - val_loss: 0.3832 - val_accuracy: 0.9000 - 181ms/epoch - 955us/step\n",
      "Epoch 4/30\n",
      "189/189 - 0s - loss: 0.2596 - accuracy: 0.9397 - val_loss: 0.3814 - val_accuracy: 0.9095 - 180ms/epoch - 950us/step\n",
      "Epoch 5/30\n",
      "189/189 - 0s - loss: 0.2589 - accuracy: 0.9439 - val_loss: 0.3825 - val_accuracy: 0.9000 - 180ms/epoch - 950us/step\n",
      "Epoch 6/30\n",
      "189/189 - 0s - loss: 0.2577 - accuracy: 0.9450 - val_loss: 0.3832 - val_accuracy: 0.9000 - 180ms/epoch - 950us/step\n",
      "Epoch 7/30\n",
      "189/189 - 0s - loss: 0.2569 - accuracy: 0.9450 - val_loss: 0.3826 - val_accuracy: 0.9000 - 178ms/epoch - 939us/step\n",
      "Epoch 8/30\n",
      "189/189 - 0s - loss: 0.2561 - accuracy: 0.9407 - val_loss: 0.3824 - val_accuracy: 0.9000 - 178ms/epoch - 943us/step\n",
      "Epoch 9/30\n",
      "189/189 - 0s - loss: 0.2554 - accuracy: 0.9444 - val_loss: 0.3824 - val_accuracy: 0.9000 - 180ms/epoch - 950us/step\n",
      "Epoch 10/30\n",
      "189/189 - 0s - loss: 0.2546 - accuracy: 0.9476 - val_loss: 0.3834 - val_accuracy: 0.9000 - 196ms/epoch - 1ms/step\n",
      "Epoch 11/30\n",
      "189/189 - 0s - loss: 0.2539 - accuracy: 0.9466 - val_loss: 0.3820 - val_accuracy: 0.9095 - 220ms/epoch - 1ms/step\n",
      "Epoch 12/30\n",
      "189/189 - 0s - loss: 0.2531 - accuracy: 0.9413 - val_loss: 0.3810 - val_accuracy: 0.9095 - 207ms/epoch - 1ms/step\n",
      "Epoch 13/30\n",
      "189/189 - 0s - loss: 0.2527 - accuracy: 0.9487 - val_loss: 0.3816 - val_accuracy: 0.9095 - 187ms/epoch - 989us/step\n",
      "Epoch 14/30\n",
      "189/189 - 0s - loss: 0.2520 - accuracy: 0.9487 - val_loss: 0.3820 - val_accuracy: 0.9095 - 186ms/epoch - 981us/step\n",
      "Epoch 15/30\n",
      "189/189 - 0s - loss: 0.2515 - accuracy: 0.9471 - val_loss: 0.3815 - val_accuracy: 0.9095 - 234ms/epoch - 1ms/step\n",
      "Epoch 16/30\n",
      "189/189 - 0s - loss: 0.2507 - accuracy: 0.9466 - val_loss: 0.3807 - val_accuracy: 0.9095 - 192ms/epoch - 1ms/step\n",
      "Epoch 17/30\n",
      "189/189 - 0s - loss: 0.2502 - accuracy: 0.9487 - val_loss: 0.3809 - val_accuracy: 0.9095 - 180ms/epoch - 950us/step\n",
      "Epoch 18/30\n",
      "189/189 - 0s - loss: 0.2497 - accuracy: 0.9471 - val_loss: 0.3819 - val_accuracy: 0.9000 - 179ms/epoch - 945us/step\n",
      "Epoch 19/30\n",
      "189/189 - 0s - loss: 0.2495 - accuracy: 0.9418 - val_loss: 0.3797 - val_accuracy: 0.9095 - 188ms/epoch - 997us/step\n",
      "Epoch 20/30\n",
      "189/189 - 0s - loss: 0.2487 - accuracy: 0.9487 - val_loss: 0.3801 - val_accuracy: 0.9095 - 179ms/epoch - 947us/step\n",
      "Epoch 21/30\n",
      "189/189 - 0s - loss: 0.2483 - accuracy: 0.9466 - val_loss: 0.3794 - val_accuracy: 0.9095 - 180ms/epoch - 954us/step\n",
      "Epoch 22/30\n",
      "189/189 - 0s - loss: 0.2479 - accuracy: 0.9487 - val_loss: 0.3807 - val_accuracy: 0.9095 - 177ms/epoch - 937us/step\n",
      "Epoch 23/30\n",
      "189/189 - 0s - loss: 0.2475 - accuracy: 0.9487 - val_loss: 0.3807 - val_accuracy: 0.9095 - 177ms/epoch - 934us/step\n",
      "Epoch 24/30\n",
      "189/189 - 0s - loss: 0.2472 - accuracy: 0.9466 - val_loss: 0.3804 - val_accuracy: 0.9095 - 177ms/epoch - 936us/step\n",
      "Epoch 25/30\n",
      "189/189 - 0s - loss: 0.2469 - accuracy: 0.9450 - val_loss: 0.3791 - val_accuracy: 0.9095 - 178ms/epoch - 942us/step\n",
      "Epoch 26/30\n",
      "189/189 - 0s - loss: 0.2463 - accuracy: 0.9487 - val_loss: 0.3785 - val_accuracy: 0.9095 - 188ms/epoch - 997us/step\n",
      "Epoch 27/30\n",
      "189/189 - 0s - loss: 0.2460 - accuracy: 0.9487 - val_loss: 0.3787 - val_accuracy: 0.9095 - 204ms/epoch - 1ms/step\n",
      "Epoch 28/30\n",
      "189/189 - 0s - loss: 0.2456 - accuracy: 0.9481 - val_loss: 0.3783 - val_accuracy: 0.9095 - 181ms/epoch - 955us/step\n",
      "Epoch 29/30\n",
      "189/189 - 0s - loss: 0.2452 - accuracy: 0.9487 - val_loss: 0.3780 - val_accuracy: 0.9095 - 182ms/epoch - 960us/step\n",
      "Epoch 30/30\n",
      "189/189 - 0s - loss: 0.2449 - accuracy: 0.9487 - val_loss: 0.3782 - val_accuracy: 0.9095 - 207ms/epoch - 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1731d1c0688>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = scaled_train_samples,\n",
    "          y = train_labels,\n",
    "          validation_split = 0.1,\n",
    "          batch_size = 10,\n",
    "          epochs = 30,\n",
    "          verbose= 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Inference?\n",
    "\n",
    " the model is using its knowledge gained from training and using it to infer a prediction or result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels =  []\n",
    "test_samples = []\n",
    "\n",
    "for i in range(10):\n",
    "    # The 5% of younger individuals who did experience side effects\n",
    "    random_younger = randint(13,64)\n",
    "    test_samples.append(random_younger)\n",
    "    test_labels.append(1)\n",
    "\n",
    "    # The 5% of older individuals who did not experience side effects\n",
    "    random_older = randint(65,100)\n",
    "    test_samples.append(random_older)\n",
    "    test_labels.append(0)\n",
    "\n",
    "for i in range(200):\n",
    "    # The 95% of younger individuals who did not experience side effects\n",
    "    random_younger = randint(13,64)\n",
    "    test_samples.append(random_younger)\n",
    "    test_labels.append(0)\n",
    "\n",
    "    # The 95% of older individuals who did experience side effects\n",
    "    random_older = randint(65,100)\n",
    "    test_samples.append(random_older)\n",
    "    test_labels.append(1)\n",
    "\n",
    "test_labels = np.array(test_labels)\n",
    "test_samples = np.array(test_samples)\n",
    "test_labels, test_samples = shuffle(test_labels, test_samples)\n",
    "\n",
    "scaled_test_samples = scalar.fit_transform(test_samples.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 40,  58,  80,  76,  99,  78,  75,  62,  29,  55,  64,  68,  59,\n",
       "        25,  79,  66,  28,  86,  27,  91,  22,  54,  39,  81, 100,  13,\n",
       "        91,  68,  81,  89,  76,  17,  16,  20,  76,  67,  87,  31,  46,\n",
       "        81,  99,  98,  85,  91,  43,  94,  93,  48,  58,  27,  65,  80,\n",
       "        58,  67,  95,  17,  74,  29,  36,  92,  93,  31,  92,  74,  72,\n",
       "        32,  66,  95,  83,  79,  72,  90,  41,  42,  55,  99,  89,  58,\n",
       "        50,  70,  29,  78,  74,  19,  75,  27,  23,  19,  88,  73,  81,\n",
       "        27,  20,  68,  91,  17,  81,  93,  48,  65,  86,  60,  29,  91,\n",
       "        72,  84,  81,  58,  67,  35,  58,  61,  86,  79,  61,  56,  66,\n",
       "        24,  90,  52,  97,  80,  47,  87,  36,  98,  64,  52,  21,  49,\n",
       "        86,  15,  81,  88,  65,  72,  61,  73,  51,  91,  61,  70,  89,\n",
       "        70,  72,  83,  83,  34,  72,  58,  28,  17,  86,  34,  39,  45,\n",
       "        79,  57,  49,  35,  71,  77,  97,  87,  25,  94,  20,  89,  80,\n",
       "        95,  66,  16,  88,  36,  43,  41,  68,  57,  78,  41,  16,  42,\n",
       "        84,  82,  87,  69,  84,  39,  25,  62,  92,  95,  89,  50,  90,\n",
       "        46,  30,  35,  17,  20,  40,  72,  61,  74,  95,  34,  87,  74,\n",
       "        60,  14,  21,  59,  39,  86, 100,  52,  93,  88,  52,  90,  16,\n",
       "        95,  14,  66,  39,  41,  66,  70,  19,  92,  52,  64,  89,  58,\n",
       "        76,  75,  43,  58,  69,  70,  57,  27,  45,  40,  31,  83,  90,\n",
       "        87,  62,  13,  66,  30,  49,  52,  95,  96,  96,  37,  77,  55,\n",
       "        62,  51,  83,  63,  81,  71,  23,  85,  60,  83,  95,  40,  96,\n",
       "        39,  54,  94,  59,  46,  37,  94,  44,  35,  56,  51,  52,  18,\n",
       "        91,  76,  54, 100,  43,  67,  80,  53,  57,  43,  74,  37,  21,\n",
       "        37,  76,  69,  33,  20,  68,  52,  87,  93,  34,  53,  25,  87,\n",
       "        74,  80,  52,  34,  96,  68,  26,  49,  76,  42,  35,  84,  36,\n",
       "        97,  51,  83,  89,  45,  88,  67,  61,  69,  75,  99,  64,  90,\n",
       "        71,  85,  25,  96,  71,  97,  70,  18,  24,  57,  87,  67,  77,\n",
       "        45,  18,  13,  34,  53,  72,  95,  80,  23,  48,  16,  52,  70,\n",
       "        45,  52,  63,  22,  33,  69,  85,  65,  26,  98,  75,  16,  20,\n",
       "        58,  81,  90,  30,  31,  42,  31,  62,  24,  36,  32,  91,  38,\n",
       "        55,  76,  97,  90,  97,  15,  95,  85,  83,  33,  36,  34,  75,\n",
       "        65,  98,  80,  55,  63,  30,  73,  40,  58,  84,  32,  97,  28,\n",
       "        70,  93,  87,  54])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 40,   1],\n",
       "       [ 58,   0],\n",
       "       [ 80,   1],\n",
       "       [ 76,   1],\n",
       "       [ 99,   1],\n",
       "       [ 78,   1],\n",
       "       [ 75,   1],\n",
       "       [ 62,   0],\n",
       "       [ 29,   0],\n",
       "       [ 55,   1],\n",
       "       [ 64,   0],\n",
       "       [ 68,   1],\n",
       "       [ 59,   0],\n",
       "       [ 25,   0],\n",
       "       [ 79,   1],\n",
       "       [ 66,   1],\n",
       "       [ 28,   0],\n",
       "       [ 86,   1],\n",
       "       [ 27,   0],\n",
       "       [ 91,   1],\n",
       "       [ 22,   0],\n",
       "       [ 54,   0],\n",
       "       [ 39,   0],\n",
       "       [ 81,   1],\n",
       "       [100,   1],\n",
       "       [ 13,   0],\n",
       "       [ 91,   1],\n",
       "       [ 68,   1],\n",
       "       [ 81,   1],\n",
       "       [ 89,   1],\n",
       "       [ 76,   1],\n",
       "       [ 17,   0],\n",
       "       [ 16,   0],\n",
       "       [ 20,   0],\n",
       "       [ 76,   1],\n",
       "       [ 67,   1],\n",
       "       [ 87,   1],\n",
       "       [ 31,   0],\n",
       "       [ 46,   0],\n",
       "       [ 81,   1],\n",
       "       [ 99,   1],\n",
       "       [ 98,   1],\n",
       "       [ 85,   1],\n",
       "       [ 91,   1],\n",
       "       [ 43,   0],\n",
       "       [ 94,   1],\n",
       "       [ 93,   1],\n",
       "       [ 48,   0],\n",
       "       [ 58,   0],\n",
       "       [ 27,   0],\n",
       "       [ 65,   1],\n",
       "       [ 80,   1],\n",
       "       [ 58,   0],\n",
       "       [ 67,   0],\n",
       "       [ 95,   1],\n",
       "       [ 17,   0],\n",
       "       [ 74,   1],\n",
       "       [ 29,   0],\n",
       "       [ 36,   0],\n",
       "       [ 92,   1],\n",
       "       [ 93,   1],\n",
       "       [ 31,   0],\n",
       "       [ 92,   1],\n",
       "       [ 74,   1],\n",
       "       [ 72,   1],\n",
       "       [ 32,   0],\n",
       "       [ 66,   1],\n",
       "       [ 95,   1],\n",
       "       [ 83,   1],\n",
       "       [ 79,   1],\n",
       "       [ 72,   1],\n",
       "       [ 90,   1],\n",
       "       [ 41,   0],\n",
       "       [ 42,   0],\n",
       "       [ 55,   0],\n",
       "       [ 99,   1],\n",
       "       [ 89,   1],\n",
       "       [ 58,   0],\n",
       "       [ 50,   0],\n",
       "       [ 70,   0],\n",
       "       [ 29,   0],\n",
       "       [ 78,   1],\n",
       "       [ 74,   1],\n",
       "       [ 19,   0],\n",
       "       [ 75,   1],\n",
       "       [ 27,   0],\n",
       "       [ 23,   0],\n",
       "       [ 19,   0],\n",
       "       [ 88,   1],\n",
       "       [ 73,   1],\n",
       "       [ 81,   1],\n",
       "       [ 27,   0],\n",
       "       [ 20,   0],\n",
       "       [ 68,   1],\n",
       "       [ 91,   1],\n",
       "       [ 17,   0],\n",
       "       [ 81,   1],\n",
       "       [ 93,   1],\n",
       "       [ 48,   0],\n",
       "       [ 65,   1],\n",
       "       [ 86,   1],\n",
       "       [ 60,   0],\n",
       "       [ 29,   0],\n",
       "       [ 91,   1],\n",
       "       [ 72,   0],\n",
       "       [ 84,   1],\n",
       "       [ 81,   0],\n",
       "       [ 58,   0],\n",
       "       [ 67,   0],\n",
       "       [ 35,   0],\n",
       "       [ 58,   0],\n",
       "       [ 61,   0],\n",
       "       [ 86,   1],\n",
       "       [ 79,   1],\n",
       "       [ 61,   0],\n",
       "       [ 56,   0],\n",
       "       [ 66,   1],\n",
       "       [ 24,   0],\n",
       "       [ 90,   1],\n",
       "       [ 52,   0],\n",
       "       [ 97,   1],\n",
       "       [ 80,   1],\n",
       "       [ 47,   0],\n",
       "       [ 87,   1],\n",
       "       [ 36,   0],\n",
       "       [ 98,   1],\n",
       "       [ 64,   0],\n",
       "       [ 52,   1],\n",
       "       [ 21,   0],\n",
       "       [ 49,   0],\n",
       "       [ 86,   1],\n",
       "       [ 15,   0],\n",
       "       [ 81,   1],\n",
       "       [ 88,   1],\n",
       "       [ 65,   1],\n",
       "       [ 72,   1],\n",
       "       [ 61,   0],\n",
       "       [ 73,   1],\n",
       "       [ 51,   0],\n",
       "       [ 91,   1],\n",
       "       [ 61,   0],\n",
       "       [ 70,   1],\n",
       "       [ 89,   1],\n",
       "       [ 70,   0],\n",
       "       [ 72,   1],\n",
       "       [ 83,   1],\n",
       "       [ 83,   1],\n",
       "       [ 34,   0],\n",
       "       [ 72,   1],\n",
       "       [ 58,   0],\n",
       "       [ 28,   0],\n",
       "       [ 17,   0],\n",
       "       [ 86,   1],\n",
       "       [ 34,   0],\n",
       "       [ 39,   0],\n",
       "       [ 45,   0],\n",
       "       [ 79,   1],\n",
       "       [ 57,   0],\n",
       "       [ 49,   0],\n",
       "       [ 35,   0],\n",
       "       [ 71,   1],\n",
       "       [ 77,   1],\n",
       "       [ 97,   1],\n",
       "       [ 87,   1],\n",
       "       [ 25,   0],\n",
       "       [ 94,   1],\n",
       "       [ 20,   1],\n",
       "       [ 89,   1],\n",
       "       [ 80,   0],\n",
       "       [ 95,   1],\n",
       "       [ 66,   1],\n",
       "       [ 16,   0],\n",
       "       [ 88,   1],\n",
       "       [ 36,   0],\n",
       "       [ 43,   0],\n",
       "       [ 41,   0],\n",
       "       [ 68,   1],\n",
       "       [ 57,   0],\n",
       "       [ 78,   1],\n",
       "       [ 41,   0],\n",
       "       [ 16,   0],\n",
       "       [ 42,   0],\n",
       "       [ 84,   1],\n",
       "       [ 82,   1],\n",
       "       [ 87,   1],\n",
       "       [ 69,   1],\n",
       "       [ 84,   1],\n",
       "       [ 39,   0],\n",
       "       [ 25,   0],\n",
       "       [ 62,   0],\n",
       "       [ 92,   1],\n",
       "       [ 95,   1],\n",
       "       [ 89,   1],\n",
       "       [ 50,   0],\n",
       "       [ 90,   1],\n",
       "       [ 46,   0],\n",
       "       [ 30,   0],\n",
       "       [ 35,   0],\n",
       "       [ 17,   0],\n",
       "       [ 20,   0],\n",
       "       [ 40,   0],\n",
       "       [ 72,   1],\n",
       "       [ 61,   0],\n",
       "       [ 74,   1],\n",
       "       [ 95,   1],\n",
       "       [ 34,   0],\n",
       "       [ 87,   1],\n",
       "       [ 74,   1],\n",
       "       [ 60,   0],\n",
       "       [ 14,   0],\n",
       "       [ 21,   0],\n",
       "       [ 59,   0],\n",
       "       [ 39,   0],\n",
       "       [ 86,   1],\n",
       "       [100,   1],\n",
       "       [ 52,   0],\n",
       "       [ 93,   1],\n",
       "       [ 88,   1],\n",
       "       [ 52,   0],\n",
       "       [ 90,   1],\n",
       "       [ 16,   0],\n",
       "       [ 95,   1],\n",
       "       [ 14,   0],\n",
       "       [ 66,   1],\n",
       "       [ 39,   0],\n",
       "       [ 41,   0],\n",
       "       [ 66,   1],\n",
       "       [ 70,   1],\n",
       "       [ 19,   0],\n",
       "       [ 92,   1],\n",
       "       [ 52,   0],\n",
       "       [ 64,   0],\n",
       "       [ 89,   1],\n",
       "       [ 58,   0],\n",
       "       [ 76,   1],\n",
       "       [ 75,   1],\n",
       "       [ 43,   0],\n",
       "       [ 58,   0],\n",
       "       [ 69,   1],\n",
       "       [ 70,   1],\n",
       "       [ 57,   0],\n",
       "       [ 27,   0],\n",
       "       [ 45,   0],\n",
       "       [ 40,   0],\n",
       "       [ 31,   0],\n",
       "       [ 83,   1],\n",
       "       [ 90,   1],\n",
       "       [ 87,   1],\n",
       "       [ 62,   0],\n",
       "       [ 13,   0],\n",
       "       [ 66,   0],\n",
       "       [ 30,   0],\n",
       "       [ 49,   0],\n",
       "       [ 52,   1],\n",
       "       [ 95,   1],\n",
       "       [ 96,   1],\n",
       "       [ 96,   1],\n",
       "       [ 37,   0],\n",
       "       [ 77,   1],\n",
       "       [ 55,   0],\n",
       "       [ 62,   0],\n",
       "       [ 51,   1],\n",
       "       [ 83,   1],\n",
       "       [ 63,   0],\n",
       "       [ 81,   0],\n",
       "       [ 71,   1],\n",
       "       [ 23,   0],\n",
       "       [ 85,   1],\n",
       "       [ 60,   1],\n",
       "       [ 83,   1],\n",
       "       [ 95,   1],\n",
       "       [ 40,   0],\n",
       "       [ 96,   1],\n",
       "       [ 39,   0],\n",
       "       [ 54,   0],\n",
       "       [ 94,   1],\n",
       "       [ 59,   0],\n",
       "       [ 46,   0],\n",
       "       [ 37,   0],\n",
       "       [ 94,   1],\n",
       "       [ 44,   0],\n",
       "       [ 35,   0],\n",
       "       [ 56,   0],\n",
       "       [ 51,   0],\n",
       "       [ 52,   0],\n",
       "       [ 18,   0],\n",
       "       [ 91,   1],\n",
       "       [ 76,   1],\n",
       "       [ 54,   0],\n",
       "       [100,   1],\n",
       "       [ 43,   1],\n",
       "       [ 67,   1],\n",
       "       [ 80,   1],\n",
       "       [ 53,   0],\n",
       "       [ 57,   0],\n",
       "       [ 43,   0],\n",
       "       [ 74,   1],\n",
       "       [ 37,   0],\n",
       "       [ 21,   0],\n",
       "       [ 37,   0],\n",
       "       [ 76,   1],\n",
       "       [ 69,   1],\n",
       "       [ 33,   0],\n",
       "       [ 20,   1],\n",
       "       [ 68,   1],\n",
       "       [ 52,   0],\n",
       "       [ 87,   1],\n",
       "       [ 93,   1],\n",
       "       [ 34,   0],\n",
       "       [ 53,   0],\n",
       "       [ 25,   0],\n",
       "       [ 87,   1],\n",
       "       [ 74,   1],\n",
       "       [ 80,   1],\n",
       "       [ 52,   0],\n",
       "       [ 34,   0],\n",
       "       [ 96,   1],\n",
       "       [ 68,   1],\n",
       "       [ 26,   0],\n",
       "       [ 49,   0],\n",
       "       [ 76,   1],\n",
       "       [ 42,   0],\n",
       "       [ 35,   0],\n",
       "       [ 84,   1],\n",
       "       [ 36,   0],\n",
       "       [ 97,   1],\n",
       "       [ 51,   0],\n",
       "       [ 83,   1],\n",
       "       [ 89,   1],\n",
       "       [ 45,   0],\n",
       "       [ 88,   1],\n",
       "       [ 67,   1],\n",
       "       [ 61,   0],\n",
       "       [ 69,   1],\n",
       "       [ 75,   1],\n",
       "       [ 99,   1],\n",
       "       [ 64,   0],\n",
       "       [ 90,   1],\n",
       "       [ 71,   1],\n",
       "       [ 85,   1],\n",
       "       [ 25,   0],\n",
       "       [ 96,   1],\n",
       "       [ 71,   1],\n",
       "       [ 97,   1],\n",
       "       [ 70,   1],\n",
       "       [ 18,   0],\n",
       "       [ 24,   0],\n",
       "       [ 57,   0],\n",
       "       [ 87,   1],\n",
       "       [ 67,   1],\n",
       "       [ 77,   1],\n",
       "       [ 45,   0],\n",
       "       [ 18,   0],\n",
       "       [ 13,   0],\n",
       "       [ 34,   0],\n",
       "       [ 53,   0],\n",
       "       [ 72,   1],\n",
       "       [ 95,   1],\n",
       "       [ 80,   1],\n",
       "       [ 23,   0],\n",
       "       [ 48,   0],\n",
       "       [ 16,   0],\n",
       "       [ 52,   0],\n",
       "       [ 70,   0],\n",
       "       [ 45,   0],\n",
       "       [ 52,   0],\n",
       "       [ 63,   0],\n",
       "       [ 22,   0],\n",
       "       [ 33,   0],\n",
       "       [ 69,   1],\n",
       "       [ 85,   1],\n",
       "       [ 65,   1],\n",
       "       [ 26,   0],\n",
       "       [ 98,   1],\n",
       "       [ 75,   1],\n",
       "       [ 16,   0],\n",
       "       [ 20,   0],\n",
       "       [ 58,   0],\n",
       "       [ 81,   1],\n",
       "       [ 90,   1],\n",
       "       [ 30,   0],\n",
       "       [ 31,   0],\n",
       "       [ 42,   0],\n",
       "       [ 31,   0],\n",
       "       [ 62,   0],\n",
       "       [ 24,   0],\n",
       "       [ 36,   0],\n",
       "       [ 32,   0],\n",
       "       [ 91,   1],\n",
       "       [ 38,   1],\n",
       "       [ 55,   0],\n",
       "       [ 76,   1],\n",
       "       [ 97,   1],\n",
       "       [ 90,   1],\n",
       "       [ 97,   1],\n",
       "       [ 15,   0],\n",
       "       [ 95,   1],\n",
       "       [ 85,   1],\n",
       "       [ 83,   1],\n",
       "       [ 33,   0],\n",
       "       [ 36,   0],\n",
       "       [ 34,   0],\n",
       "       [ 75,   1],\n",
       "       [ 65,   1],\n",
       "       [ 98,   1],\n",
       "       [ 80,   1],\n",
       "       [ 55,   0],\n",
       "       [ 63,   0],\n",
       "       [ 30,   0],\n",
       "       [ 73,   1],\n",
       "       [ 40,   0],\n",
       "       [ 58,   0],\n",
       "       [ 84,   1],\n",
       "       [ 32,   0],\n",
       "       [ 97,   1],\n",
       "       [ 28,   0],\n",
       "       [ 70,   1],\n",
       "       [ 93,   1],\n",
       "       [ 87,   1],\n",
       "       [ 54,   0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels = np.array(test_labels)\n",
    "tesn_samples = np.array(test_samples)\n",
    "np.c_[test_samples.reshape(420,1), test_labels.reshape(420,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31034483],\n",
       "       [0.51724138],\n",
       "       [0.77011494],\n",
       "       [0.72413793],\n",
       "       [0.98850575],\n",
       "       [0.74712644],\n",
       "       [0.71264368],\n",
       "       [0.56321839],\n",
       "       [0.18390805],\n",
       "       [0.48275862],\n",
       "       [0.5862069 ],\n",
       "       [0.63218391],\n",
       "       [0.52873563],\n",
       "       [0.13793103],\n",
       "       [0.75862069],\n",
       "       [0.6091954 ],\n",
       "       [0.17241379],\n",
       "       [0.83908046],\n",
       "       [0.16091954],\n",
       "       [0.89655172],\n",
       "       [0.10344828],\n",
       "       [0.47126437],\n",
       "       [0.29885057],\n",
       "       [0.7816092 ],\n",
       "       [1.        ],\n",
       "       [0.        ],\n",
       "       [0.89655172],\n",
       "       [0.63218391],\n",
       "       [0.7816092 ],\n",
       "       [0.87356322],\n",
       "       [0.72413793],\n",
       "       [0.04597701],\n",
       "       [0.03448276],\n",
       "       [0.08045977],\n",
       "       [0.72413793],\n",
       "       [0.62068966],\n",
       "       [0.85057471],\n",
       "       [0.20689655],\n",
       "       [0.37931034],\n",
       "       [0.7816092 ],\n",
       "       [0.98850575],\n",
       "       [0.97701149],\n",
       "       [0.82758621],\n",
       "       [0.89655172],\n",
       "       [0.34482759],\n",
       "       [0.93103448],\n",
       "       [0.91954023],\n",
       "       [0.40229885],\n",
       "       [0.51724138],\n",
       "       [0.16091954],\n",
       "       [0.59770115],\n",
       "       [0.77011494],\n",
       "       [0.51724138],\n",
       "       [0.62068966],\n",
       "       [0.94252874],\n",
       "       [0.04597701],\n",
       "       [0.70114943],\n",
       "       [0.18390805],\n",
       "       [0.26436782],\n",
       "       [0.90804598],\n",
       "       [0.91954023],\n",
       "       [0.20689655],\n",
       "       [0.90804598],\n",
       "       [0.70114943],\n",
       "       [0.67816092],\n",
       "       [0.2183908 ],\n",
       "       [0.6091954 ],\n",
       "       [0.94252874],\n",
       "       [0.8045977 ],\n",
       "       [0.75862069],\n",
       "       [0.67816092],\n",
       "       [0.88505747],\n",
       "       [0.32183908],\n",
       "       [0.33333333],\n",
       "       [0.48275862],\n",
       "       [0.98850575],\n",
       "       [0.87356322],\n",
       "       [0.51724138],\n",
       "       [0.42528736],\n",
       "       [0.65517241],\n",
       "       [0.18390805],\n",
       "       [0.74712644],\n",
       "       [0.70114943],\n",
       "       [0.06896552],\n",
       "       [0.71264368],\n",
       "       [0.16091954],\n",
       "       [0.11494253],\n",
       "       [0.06896552],\n",
       "       [0.86206897],\n",
       "       [0.68965517],\n",
       "       [0.7816092 ],\n",
       "       [0.16091954],\n",
       "       [0.08045977],\n",
       "       [0.63218391],\n",
       "       [0.89655172],\n",
       "       [0.04597701],\n",
       "       [0.7816092 ],\n",
       "       [0.91954023],\n",
       "       [0.40229885],\n",
       "       [0.59770115],\n",
       "       [0.83908046],\n",
       "       [0.54022989],\n",
       "       [0.18390805],\n",
       "       [0.89655172],\n",
       "       [0.67816092],\n",
       "       [0.81609195],\n",
       "       [0.7816092 ],\n",
       "       [0.51724138],\n",
       "       [0.62068966],\n",
       "       [0.25287356],\n",
       "       [0.51724138],\n",
       "       [0.55172414],\n",
       "       [0.83908046],\n",
       "       [0.75862069],\n",
       "       [0.55172414],\n",
       "       [0.49425287],\n",
       "       [0.6091954 ],\n",
       "       [0.12643678],\n",
       "       [0.88505747],\n",
       "       [0.44827586],\n",
       "       [0.96551724],\n",
       "       [0.77011494],\n",
       "       [0.3908046 ],\n",
       "       [0.85057471],\n",
       "       [0.26436782],\n",
       "       [0.97701149],\n",
       "       [0.5862069 ],\n",
       "       [0.44827586],\n",
       "       [0.09195402],\n",
       "       [0.4137931 ],\n",
       "       [0.83908046],\n",
       "       [0.02298851],\n",
       "       [0.7816092 ],\n",
       "       [0.86206897],\n",
       "       [0.59770115],\n",
       "       [0.67816092],\n",
       "       [0.55172414],\n",
       "       [0.68965517],\n",
       "       [0.43678161],\n",
       "       [0.89655172],\n",
       "       [0.55172414],\n",
       "       [0.65517241],\n",
       "       [0.87356322],\n",
       "       [0.65517241],\n",
       "       [0.67816092],\n",
       "       [0.8045977 ],\n",
       "       [0.8045977 ],\n",
       "       [0.24137931],\n",
       "       [0.67816092],\n",
       "       [0.51724138],\n",
       "       [0.17241379],\n",
       "       [0.04597701],\n",
       "       [0.83908046],\n",
       "       [0.24137931],\n",
       "       [0.29885057],\n",
       "       [0.36781609],\n",
       "       [0.75862069],\n",
       "       [0.50574713],\n",
       "       [0.4137931 ],\n",
       "       [0.25287356],\n",
       "       [0.66666667],\n",
       "       [0.73563218],\n",
       "       [0.96551724],\n",
       "       [0.85057471],\n",
       "       [0.13793103],\n",
       "       [0.93103448],\n",
       "       [0.08045977],\n",
       "       [0.87356322],\n",
       "       [0.77011494],\n",
       "       [0.94252874],\n",
       "       [0.6091954 ],\n",
       "       [0.03448276],\n",
       "       [0.86206897],\n",
       "       [0.26436782],\n",
       "       [0.34482759],\n",
       "       [0.32183908],\n",
       "       [0.63218391],\n",
       "       [0.50574713],\n",
       "       [0.74712644],\n",
       "       [0.32183908],\n",
       "       [0.03448276],\n",
       "       [0.33333333],\n",
       "       [0.81609195],\n",
       "       [0.79310345],\n",
       "       [0.85057471],\n",
       "       [0.64367816],\n",
       "       [0.81609195],\n",
       "       [0.29885057],\n",
       "       [0.13793103],\n",
       "       [0.56321839],\n",
       "       [0.90804598],\n",
       "       [0.94252874],\n",
       "       [0.87356322],\n",
       "       [0.42528736],\n",
       "       [0.88505747],\n",
       "       [0.37931034],\n",
       "       [0.1954023 ],\n",
       "       [0.25287356],\n",
       "       [0.04597701],\n",
       "       [0.08045977],\n",
       "       [0.31034483],\n",
       "       [0.67816092],\n",
       "       [0.55172414],\n",
       "       [0.70114943],\n",
       "       [0.94252874],\n",
       "       [0.24137931],\n",
       "       [0.85057471],\n",
       "       [0.70114943],\n",
       "       [0.54022989],\n",
       "       [0.01149425],\n",
       "       [0.09195402],\n",
       "       [0.52873563],\n",
       "       [0.29885057],\n",
       "       [0.83908046],\n",
       "       [1.        ],\n",
       "       [0.44827586],\n",
       "       [0.91954023],\n",
       "       [0.86206897],\n",
       "       [0.44827586],\n",
       "       [0.88505747],\n",
       "       [0.03448276],\n",
       "       [0.94252874],\n",
       "       [0.01149425],\n",
       "       [0.6091954 ],\n",
       "       [0.29885057],\n",
       "       [0.32183908],\n",
       "       [0.6091954 ],\n",
       "       [0.65517241],\n",
       "       [0.06896552],\n",
       "       [0.90804598],\n",
       "       [0.44827586],\n",
       "       [0.5862069 ],\n",
       "       [0.87356322],\n",
       "       [0.51724138],\n",
       "       [0.72413793],\n",
       "       [0.71264368],\n",
       "       [0.34482759],\n",
       "       [0.51724138],\n",
       "       [0.64367816],\n",
       "       [0.65517241],\n",
       "       [0.50574713],\n",
       "       [0.16091954],\n",
       "       [0.36781609],\n",
       "       [0.31034483],\n",
       "       [0.20689655],\n",
       "       [0.8045977 ],\n",
       "       [0.88505747],\n",
       "       [0.85057471],\n",
       "       [0.56321839],\n",
       "       [0.        ],\n",
       "       [0.6091954 ],\n",
       "       [0.1954023 ],\n",
       "       [0.4137931 ],\n",
       "       [0.44827586],\n",
       "       [0.94252874],\n",
       "       [0.95402299],\n",
       "       [0.95402299],\n",
       "       [0.27586207],\n",
       "       [0.73563218],\n",
       "       [0.48275862],\n",
       "       [0.56321839],\n",
       "       [0.43678161],\n",
       "       [0.8045977 ],\n",
       "       [0.57471264],\n",
       "       [0.7816092 ],\n",
       "       [0.66666667],\n",
       "       [0.11494253],\n",
       "       [0.82758621],\n",
       "       [0.54022989],\n",
       "       [0.8045977 ],\n",
       "       [0.94252874],\n",
       "       [0.31034483],\n",
       "       [0.95402299],\n",
       "       [0.29885057],\n",
       "       [0.47126437],\n",
       "       [0.93103448],\n",
       "       [0.52873563],\n",
       "       [0.37931034],\n",
       "       [0.27586207],\n",
       "       [0.93103448],\n",
       "       [0.35632184],\n",
       "       [0.25287356],\n",
       "       [0.49425287],\n",
       "       [0.43678161],\n",
       "       [0.44827586],\n",
       "       [0.05747126],\n",
       "       [0.89655172],\n",
       "       [0.72413793],\n",
       "       [0.47126437],\n",
       "       [1.        ],\n",
       "       [0.34482759],\n",
       "       [0.62068966],\n",
       "       [0.77011494],\n",
       "       [0.45977011],\n",
       "       [0.50574713],\n",
       "       [0.34482759],\n",
       "       [0.70114943],\n",
       "       [0.27586207],\n",
       "       [0.09195402],\n",
       "       [0.27586207],\n",
       "       [0.72413793],\n",
       "       [0.64367816],\n",
       "       [0.22988506],\n",
       "       [0.08045977],\n",
       "       [0.63218391],\n",
       "       [0.44827586],\n",
       "       [0.85057471],\n",
       "       [0.91954023],\n",
       "       [0.24137931],\n",
       "       [0.45977011],\n",
       "       [0.13793103],\n",
       "       [0.85057471],\n",
       "       [0.70114943],\n",
       "       [0.77011494],\n",
       "       [0.44827586],\n",
       "       [0.24137931],\n",
       "       [0.95402299],\n",
       "       [0.63218391],\n",
       "       [0.14942529],\n",
       "       [0.4137931 ],\n",
       "       [0.72413793],\n",
       "       [0.33333333],\n",
       "       [0.25287356],\n",
       "       [0.81609195],\n",
       "       [0.26436782],\n",
       "       [0.96551724],\n",
       "       [0.43678161],\n",
       "       [0.8045977 ],\n",
       "       [0.87356322],\n",
       "       [0.36781609],\n",
       "       [0.86206897],\n",
       "       [0.62068966],\n",
       "       [0.55172414],\n",
       "       [0.64367816],\n",
       "       [0.71264368],\n",
       "       [0.98850575],\n",
       "       [0.5862069 ],\n",
       "       [0.88505747],\n",
       "       [0.66666667],\n",
       "       [0.82758621],\n",
       "       [0.13793103],\n",
       "       [0.95402299],\n",
       "       [0.66666667],\n",
       "       [0.96551724],\n",
       "       [0.65517241],\n",
       "       [0.05747126],\n",
       "       [0.12643678],\n",
       "       [0.50574713],\n",
       "       [0.85057471],\n",
       "       [0.62068966],\n",
       "       [0.73563218],\n",
       "       [0.36781609],\n",
       "       [0.05747126],\n",
       "       [0.        ],\n",
       "       [0.24137931],\n",
       "       [0.45977011],\n",
       "       [0.67816092],\n",
       "       [0.94252874],\n",
       "       [0.77011494],\n",
       "       [0.11494253],\n",
       "       [0.40229885],\n",
       "       [0.03448276],\n",
       "       [0.44827586],\n",
       "       [0.65517241],\n",
       "       [0.36781609],\n",
       "       [0.44827586],\n",
       "       [0.57471264],\n",
       "       [0.10344828],\n",
       "       [0.22988506],\n",
       "       [0.64367816],\n",
       "       [0.82758621],\n",
       "       [0.59770115],\n",
       "       [0.14942529],\n",
       "       [0.97701149],\n",
       "       [0.71264368],\n",
       "       [0.03448276],\n",
       "       [0.08045977],\n",
       "       [0.51724138],\n",
       "       [0.7816092 ],\n",
       "       [0.88505747],\n",
       "       [0.1954023 ],\n",
       "       [0.20689655],\n",
       "       [0.33333333],\n",
       "       [0.20689655],\n",
       "       [0.56321839],\n",
       "       [0.12643678],\n",
       "       [0.26436782],\n",
       "       [0.2183908 ],\n",
       "       [0.89655172],\n",
       "       [0.28735632],\n",
       "       [0.48275862],\n",
       "       [0.72413793],\n",
       "       [0.96551724],\n",
       "       [0.88505747],\n",
       "       [0.96551724],\n",
       "       [0.02298851],\n",
       "       [0.94252874],\n",
       "       [0.82758621],\n",
       "       [0.8045977 ],\n",
       "       [0.22988506],\n",
       "       [0.26436782],\n",
       "       [0.24137931],\n",
       "       [0.71264368],\n",
       "       [0.59770115],\n",
       "       [0.97701149],\n",
       "       [0.77011494],\n",
       "       [0.48275862],\n",
       "       [0.57471264],\n",
       "       [0.1954023 ],\n",
       "       [0.68965517],\n",
       "       [0.31034483],\n",
       "       [0.51724138],\n",
       "       [0.81609195],\n",
       "       [0.2183908 ],\n",
       "       [0.96551724],\n",
       "       [0.17241379],\n",
       "       [0.65517241],\n",
       "       [0.91954023],\n",
       "       [0.85057471],\n",
       "       [0.47126437]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_test_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating The Test Set\n",
    "To get predictions from the model for the test set, we call *model.predict()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x = scaled_test_samples,\n",
    "                            batch_size = 10,\n",
    "                            verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9538423 , 0.04615769],\n",
       "       [0.73307973, 0.26692027],\n",
       "       [0.06590642, 0.9340936 ],\n",
       "       [0.10659052, 0.89340955],\n",
       "       [0.00641842, 0.99358165],\n",
       "       [0.08336972, 0.91663027],\n",
       "       [0.12021804, 0.8797819 ],\n",
       "       [0.5681617 , 0.43183827],\n",
       "       [0.96043944, 0.03956047],\n",
       "       [0.8266782 , 0.17332174],\n",
       "       [0.47661218, 0.5233878 ],\n",
       "       [0.30373642, 0.6962636 ],\n",
       "       [0.6955756 , 0.3044245 ],\n",
       "       [0.9605715 , 0.03942839],\n",
       "       [0.07408917, 0.9259109 ],\n",
       "       [0.3866078 , 0.6133922 ],\n",
       "       [0.9604726 , 0.03952743],\n",
       "       [0.03209819, 0.9679019 ],\n",
       "       [0.96050555, 0.03949438],\n",
       "       [0.01736997, 0.98262995],\n",
       "       [0.96067035, 0.03932961],\n",
       "       [0.85147953, 0.1485204 ],\n",
       "       [0.9550323 , 0.04496771],\n",
       "       [0.05857028, 0.94142973],\n",
       "       [0.00566382, 0.9943362 ],\n",
       "       [0.9608974 , 0.03910262],\n",
       "       [0.01736997, 0.98262995],\n",
       "       [0.30373642, 0.6962636 ],\n",
       "       [0.05857028, 0.94142973],\n",
       "       [0.02223011, 0.9777699 ],\n",
       "       [0.10659052, 0.89340955],\n",
       "       [0.9608345 , 0.03916549],\n",
       "       [0.9608673 , 0.03913274],\n",
       "       [0.96073616, 0.03926388],\n",
       "       [0.10659052, 0.89340955],\n",
       "       [0.3439861 , 0.65601385],\n",
       "       [0.02841082, 0.9715892 ],\n",
       "       [0.9603733 , 0.03962669],\n",
       "       [0.9374204 , 0.06257959],\n",
       "       [0.05857028, 0.94142973],\n",
       "       [0.00641842, 0.99358165],\n",
       "       [0.00727282, 0.99272716],\n",
       "       [0.03624627, 0.96375376],\n",
       "       [0.01736997, 0.98262995],\n",
       "       [0.94659793, 0.05340208],\n",
       "       [0.01197383, 0.9880262 ],\n",
       "       [0.01355766, 0.9864423 ],\n",
       "       [0.9305049 , 0.0694951 ],\n",
       "       [0.73307973, 0.26692027],\n",
       "       [0.96050555, 0.03949438],\n",
       "       [0.43104073, 0.5689593 ],\n",
       "       [0.06590642, 0.9340936 ],\n",
       "       [0.73307973, 0.26692027],\n",
       "       [0.3439861 , 0.65601385],\n",
       "       [0.01057306, 0.989427  ],\n",
       "       [0.9608345 , 0.03916549],\n",
       "       [0.13532408, 0.8646759 ],\n",
       "       [0.96043944, 0.03956047],\n",
       "       [0.9584293 , 0.04157079],\n",
       "       [0.01534773, 0.9846523 ],\n",
       "       [0.01355766, 0.9864423 ],\n",
       "       [0.9603733 , 0.03962669],\n",
       "       [0.01534773, 0.9846523 ],\n",
       "       [0.13532408, 0.8646759 ],\n",
       "       [0.17286699, 0.82713306],\n",
       "       [0.9603402 , 0.03965983],\n",
       "       [0.3866078 , 0.6133922 ],\n",
       "       [0.01057306, 0.989427  ],\n",
       "       [0.04614003, 0.95386   ],\n",
       "       [0.07408917, 0.9259109 ],\n",
       "       [0.17286699, 0.82713306],\n",
       "       [0.01965335, 0.9803467 ],\n",
       "       [0.9519946 , 0.04800537],\n",
       "       [0.94936436, 0.05063571],\n",
       "       [0.8266782 , 0.17332174],\n",
       "       [0.00641842, 0.99358165],\n",
       "       [0.02223011, 0.9777699 ],\n",
       "       [0.73307973, 0.26692027],\n",
       "       [0.91915596, 0.08084399],\n",
       "       [0.23191269, 0.7680873 ],\n",
       "       [0.96043944, 0.03956047],\n",
       "       [0.08336972, 0.91663027],\n",
       "       [0.13532408, 0.8646759 ],\n",
       "       [0.96076894, 0.03923107],\n",
       "       [0.12021804, 0.8797819 ],\n",
       "       [0.96050555, 0.03949438],\n",
       "       [0.96063745, 0.03936251],\n",
       "       [0.96076894, 0.03923107],\n",
       "       [0.02513605, 0.97486395],\n",
       "       [0.15234801, 0.84765196],\n",
       "       [0.05857028, 0.94142973],\n",
       "       [0.96050555, 0.03949438],\n",
       "       [0.96073616, 0.03926388],\n",
       "       [0.30373642, 0.6962636 ],\n",
       "       [0.01736997, 0.98262995],\n",
       "       [0.9608345 , 0.03916549],\n",
       "       [0.05857028, 0.94142973],\n",
       "       [0.01355766, 0.9864423 ],\n",
       "       [0.9305049 , 0.0694951 ],\n",
       "       [0.43104073, 0.5689593 ],\n",
       "       [0.03209819, 0.9679019 ],\n",
       "       [0.65528   , 0.34472004],\n",
       "       [0.96043944, 0.03956047],\n",
       "       [0.01736997, 0.98262995],\n",
       "       [0.17286699, 0.82713306],\n",
       "       [0.04090774, 0.9590922 ],\n",
       "       [0.05857028, 0.94142973],\n",
       "       [0.73307973, 0.26692027],\n",
       "       [0.3439861 , 0.65601385],\n",
       "       [0.9595059 , 0.04049404],\n",
       "       [0.73307973, 0.26692027],\n",
       "       [0.6126209 , 0.38737908],\n",
       "       [0.03209819, 0.9679019 ],\n",
       "       [0.07408917, 0.9259109 ],\n",
       "       [0.6126209 , 0.38737908],\n",
       "       [0.7987144 , 0.20128557],\n",
       "       [0.3866078 , 0.6133922 ],\n",
       "       [0.96060455, 0.03939544],\n",
       "       [0.01965335, 0.9803467 ],\n",
       "       [0.8922783 , 0.10772167],\n",
       "       [0.00824   , 0.9917601 ],\n",
       "       [0.06590642, 0.9340936 ],\n",
       "       [0.9340468 , 0.06595316],\n",
       "       [0.02841082, 0.9715892 ],\n",
       "       [0.9584293 , 0.04157079],\n",
       "       [0.00727282, 0.99272716],\n",
       "       [0.47661218, 0.5233878 ],\n",
       "       [0.8922783 , 0.10772167],\n",
       "       [0.96070325, 0.03929674],\n",
       "       [0.9263545 , 0.07364551],\n",
       "       [0.03209819, 0.9679019 ],\n",
       "       [0.9608974 , 0.03910262],\n",
       "       [0.05857028, 0.94142973],\n",
       "       [0.02513605, 0.97486395],\n",
       "       [0.43104073, 0.5689593 ],\n",
       "       [0.17286699, 0.82713306],\n",
       "       [0.6126209 , 0.38737908],\n",
       "       [0.15234801, 0.84765196],\n",
       "       [0.90850264, 0.09149738],\n",
       "       [0.01736997, 0.98262995],\n",
       "       [0.6126209 , 0.38737908],\n",
       "       [0.23191269, 0.7680873 ],\n",
       "       [0.02223011, 0.9777699 ],\n",
       "       [0.23191269, 0.7680873 ],\n",
       "       [0.17286699, 0.82713306],\n",
       "       [0.04614003, 0.95386   ],\n",
       "       [0.04614003, 0.95386   ],\n",
       "       [0.9602563 , 0.03974369],\n",
       "       [0.17286699, 0.82713306],\n",
       "       [0.73307973, 0.26692027],\n",
       "       [0.9604726 , 0.03952743],\n",
       "       [0.9608345 , 0.03916549],\n",
       "       [0.03209819, 0.9679019 ],\n",
       "       [0.9602563 , 0.03974369],\n",
       "       [0.9550323 , 0.04496771],\n",
       "       [0.94063234, 0.05936763],\n",
       "       [0.07408917, 0.9259109 ],\n",
       "       [0.7675076 , 0.23249234],\n",
       "       [0.9263545 , 0.07364551],\n",
       "       [0.9595059 , 0.04049404],\n",
       "       [0.20076336, 0.79923666],\n",
       "       [0.09434202, 0.90565795],\n",
       "       [0.00824   , 0.9917601 ],\n",
       "       [0.02841082, 0.9715892 ],\n",
       "       [0.9605715 , 0.03942839],\n",
       "       [0.01197383, 0.9880262 ],\n",
       "       [0.96073616, 0.03926388],\n",
       "       [0.02223011, 0.9777699 ],\n",
       "       [0.06590642, 0.9340936 ],\n",
       "       [0.01057306, 0.989427  ],\n",
       "       [0.3866078 , 0.6133922 ],\n",
       "       [0.9608673 , 0.03913274],\n",
       "       [0.02513605, 0.97486395],\n",
       "       [0.9584293 , 0.04157079],\n",
       "       [0.94659793, 0.05340208],\n",
       "       [0.9519946 , 0.04800537],\n",
       "       [0.30373642, 0.6962636 ],\n",
       "       [0.7675076 , 0.23249234],\n",
       "       [0.08336972, 0.91663027],\n",
       "       [0.9519946 , 0.04800537],\n",
       "       [0.9608673 , 0.03913274],\n",
       "       [0.94936436, 0.05063571],\n",
       "       [0.04090774, 0.9590922 ],\n",
       "       [0.05200525, 0.9479947 ],\n",
       "       [0.02841082, 0.9715892 ],\n",
       "       [0.2662845 , 0.73371553],\n",
       "       [0.04090774, 0.9590922 ],\n",
       "       [0.9550323 , 0.04496771],\n",
       "       [0.9605715 , 0.03942839],\n",
       "       [0.5681617 , 0.43183827],\n",
       "       [0.01534773, 0.9846523 ],\n",
       "       [0.01057306, 0.989427  ],\n",
       "       [0.02223011, 0.9777699 ],\n",
       "       [0.91915596, 0.08084399],\n",
       "       [0.01965335, 0.9803467 ],\n",
       "       [0.9374204 , 0.06257959],\n",
       "       [0.96040636, 0.03959357],\n",
       "       [0.9595059 , 0.04049404],\n",
       "       [0.9608345 , 0.03916549],\n",
       "       [0.96073616, 0.03926388],\n",
       "       [0.9538423 , 0.04615769],\n",
       "       [0.17286699, 0.82713306],\n",
       "       [0.6126209 , 0.38737908],\n",
       "       [0.13532408, 0.8646759 ],\n",
       "       [0.01057306, 0.989427  ],\n",
       "       [0.9602563 , 0.03974369],\n",
       "       [0.02841082, 0.9715892 ],\n",
       "       [0.13532408, 0.8646759 ],\n",
       "       [0.65528   , 0.34472004],\n",
       "       [0.9608974 , 0.03910262],\n",
       "       [0.96070325, 0.03929674],\n",
       "       [0.6955756 , 0.3044245 ],\n",
       "       [0.9550323 , 0.04496771],\n",
       "       [0.03209819, 0.9679019 ],\n",
       "       [0.00566382, 0.9943362 ],\n",
       "       [0.8922783 , 0.10772167],\n",
       "       [0.01355766, 0.9864423 ],\n",
       "       [0.02513605, 0.97486395],\n",
       "       [0.8922783 , 0.10772167],\n",
       "       [0.01965335, 0.9803467 ],\n",
       "       [0.9608673 , 0.03913274],\n",
       "       [0.01057306, 0.989427  ],\n",
       "       [0.9608974 , 0.03910262],\n",
       "       [0.3866078 , 0.6133922 ],\n",
       "       [0.9550323 , 0.04496771],\n",
       "       [0.9519946 , 0.04800537],\n",
       "       [0.3866078 , 0.6133922 ],\n",
       "       [0.23191269, 0.7680873 ],\n",
       "       [0.96076894, 0.03923107],\n",
       "       [0.01534773, 0.9846523 ],\n",
       "       [0.8922783 , 0.10772167],\n",
       "       [0.47661218, 0.5233878 ],\n",
       "       [0.02223011, 0.9777699 ],\n",
       "       [0.73307973, 0.26692027],\n",
       "       [0.10659052, 0.89340955],\n",
       "       [0.12021804, 0.8797819 ],\n",
       "       [0.94659793, 0.05340208],\n",
       "       [0.73307973, 0.26692027],\n",
       "       [0.2662845 , 0.73371553],\n",
       "       [0.23191269, 0.7680873 ],\n",
       "       [0.7675076 , 0.23249234],\n",
       "       [0.96050555, 0.03949438],\n",
       "       [0.94063234, 0.05936763],\n",
       "       [0.9538423 , 0.04615769],\n",
       "       [0.9603733 , 0.03962669],\n",
       "       [0.04614003, 0.95386   ],\n",
       "       [0.01965335, 0.9803467 ],\n",
       "       [0.02841082, 0.9715892 ],\n",
       "       [0.5681617 , 0.43183827],\n",
       "       [0.9608974 , 0.03910262],\n",
       "       [0.3866078 , 0.6133922 ],\n",
       "       [0.96040636, 0.03959357],\n",
       "       [0.9263545 , 0.07364551],\n",
       "       [0.8922783 , 0.10772167],\n",
       "       [0.01057306, 0.989427  ],\n",
       "       [0.0093346 , 0.99066544],\n",
       "       [0.0093346 , 0.99066544],\n",
       "       [0.9573251 , 0.0426749 ],\n",
       "       [0.09434202, 0.90565795],\n",
       "       [0.8266782 , 0.17332174],\n",
       "       [0.5681617 , 0.43183827],\n",
       "       [0.90850264, 0.09149738],\n",
       "       [0.04614003, 0.95386   ],\n",
       "       [0.5225767 , 0.4774233 ],\n",
       "       [0.05857028, 0.94142973],\n",
       "       [0.20076336, 0.79923666],\n",
       "       [0.96063745, 0.03936251],\n",
       "       [0.03624627, 0.96375376],\n",
       "       [0.65528   , 0.34472004],\n",
       "       [0.04614003, 0.95386   ],\n",
       "       [0.01057306, 0.989427  ],\n",
       "       [0.9538423 , 0.04615769],\n",
       "       [0.0093346 , 0.99066544],\n",
       "       [0.9550323 , 0.04496771],\n",
       "       [0.85147953, 0.1485204 ],\n",
       "       [0.01197383, 0.9880262 ],\n",
       "       [0.6955756 , 0.3044245 ],\n",
       "       [0.9374204 , 0.06257959],\n",
       "       [0.9573251 , 0.0426749 ],\n",
       "       [0.01197383, 0.9880262 ],\n",
       "       [0.9436894 , 0.05631061],\n",
       "       [0.9595059 , 0.04049404],\n",
       "       [0.7987144 , 0.20128557],\n",
       "       [0.90850264, 0.09149738],\n",
       "       [0.8922783 , 0.10772167],\n",
       "       [0.9608017 , 0.03919826],\n",
       "       [0.01736997, 0.98262995],\n",
       "       [0.10659052, 0.89340955],\n",
       "       [0.85147953, 0.1485204 ],\n",
       "       [0.00566382, 0.9943362 ],\n",
       "       [0.94659793, 0.05340208],\n",
       "       [0.3439861 , 0.65601385],\n",
       "       [0.06590642, 0.9340936 ],\n",
       "       [0.873276  , 0.126724  ],\n",
       "       [0.7675076 , 0.23249234],\n",
       "       [0.94659793, 0.05340208],\n",
       "       [0.13532408, 0.8646759 ],\n",
       "       [0.9573251 , 0.0426749 ],\n",
       "       [0.96070325, 0.03929674],\n",
       "       [0.9573251 , 0.0426749 ],\n",
       "       [0.10659052, 0.89340955],\n",
       "       [0.2662845 , 0.73371553],\n",
       "       [0.960307  , 0.039693  ],\n",
       "       [0.96073616, 0.03926388],\n",
       "       [0.30373642, 0.6962636 ],\n",
       "       [0.8922783 , 0.10772167],\n",
       "       [0.02841082, 0.9715892 ],\n",
       "       [0.01355766, 0.9864423 ],\n",
       "       [0.9602563 , 0.03974369],\n",
       "       [0.873276  , 0.126724  ],\n",
       "       [0.9605715 , 0.03942839],\n",
       "       [0.02841082, 0.9715892 ],\n",
       "       [0.13532408, 0.8646759 ],\n",
       "       [0.06590642, 0.9340936 ],\n",
       "       [0.8922783 , 0.10772167],\n",
       "       [0.9602563 , 0.03974369],\n",
       "       [0.0093346 , 0.99066544],\n",
       "       [0.30373642, 0.6962636 ],\n",
       "       [0.96053857, 0.03946139],\n",
       "       [0.9263545 , 0.07364551],\n",
       "       [0.10659052, 0.89340955],\n",
       "       [0.94936436, 0.05063571],\n",
       "       [0.9595059 , 0.04049404],\n",
       "       [0.04090774, 0.9590922 ],\n",
       "       [0.9584293 , 0.04157079],\n",
       "       [0.00824   , 0.9917601 ],\n",
       "       [0.90850264, 0.09149738],\n",
       "       [0.04614003, 0.95386   ],\n",
       "       [0.02223011, 0.9777699 ],\n",
       "       [0.94063234, 0.05936763],\n",
       "       [0.02513605, 0.97486395],\n",
       "       [0.3439861 , 0.65601385],\n",
       "       [0.6126209 , 0.38737908],\n",
       "       [0.2662845 , 0.73371553],\n",
       "       [0.12021804, 0.8797819 ],\n",
       "       [0.00641842, 0.99358165],\n",
       "       [0.47661218, 0.5233878 ],\n",
       "       [0.01965335, 0.9803467 ],\n",
       "       [0.20076336, 0.79923666],\n",
       "       [0.03624627, 0.96375376],\n",
       "       [0.9605715 , 0.03942839],\n",
       "       [0.0093346 , 0.99066544],\n",
       "       [0.20076336, 0.79923666],\n",
       "       [0.00824   , 0.9917601 ],\n",
       "       [0.23191269, 0.7680873 ],\n",
       "       [0.9608017 , 0.03919826],\n",
       "       [0.96060455, 0.03939544],\n",
       "       [0.7675076 , 0.23249234],\n",
       "       [0.02841082, 0.9715892 ],\n",
       "       [0.3439861 , 0.65601385],\n",
       "       [0.09434202, 0.90565795],\n",
       "       [0.94063234, 0.05936763],\n",
       "       [0.9608017 , 0.03919826],\n",
       "       [0.9608974 , 0.03910262],\n",
       "       [0.9602563 , 0.03974369],\n",
       "       [0.873276  , 0.126724  ],\n",
       "       [0.17286699, 0.82713306],\n",
       "       [0.01057306, 0.989427  ],\n",
       "       [0.06590642, 0.9340936 ],\n",
       "       [0.96063745, 0.03936251],\n",
       "       [0.9305049 , 0.0694951 ],\n",
       "       [0.9608673 , 0.03913274],\n",
       "       [0.8922783 , 0.10772167],\n",
       "       [0.23191269, 0.7680873 ],\n",
       "       [0.94063234, 0.05936763],\n",
       "       [0.8922783 , 0.10772167],\n",
       "       [0.5225767 , 0.4774233 ],\n",
       "       [0.96067035, 0.03932961],\n",
       "       [0.960307  , 0.039693  ],\n",
       "       [0.2662845 , 0.73371553],\n",
       "       [0.03624627, 0.96375376],\n",
       "       [0.43104073, 0.5689593 ],\n",
       "       [0.96053857, 0.03946139],\n",
       "       [0.00727282, 0.99272716],\n",
       "       [0.12021804, 0.8797819 ],\n",
       "       [0.9608673 , 0.03913274],\n",
       "       [0.96073616, 0.03926388],\n",
       "       [0.73307973, 0.26692027],\n",
       "       [0.05857028, 0.94142973],\n",
       "       [0.01965335, 0.9803467 ],\n",
       "       [0.96040636, 0.03959357],\n",
       "       [0.9603733 , 0.03962669],\n",
       "       [0.94936436, 0.05063571],\n",
       "       [0.9603733 , 0.03962669],\n",
       "       [0.5681617 , 0.43183827],\n",
       "       [0.96060455, 0.03939544],\n",
       "       [0.9584293 , 0.04157079],\n",
       "       [0.9603402 , 0.03965983],\n",
       "       [0.01736997, 0.98262995],\n",
       "       [0.95619303, 0.04380699],\n",
       "       [0.8266782 , 0.17332174],\n",
       "       [0.10659052, 0.89340955],\n",
       "       [0.00824   , 0.9917601 ],\n",
       "       [0.01965335, 0.9803467 ],\n",
       "       [0.00824   , 0.9917601 ],\n",
       "       [0.9608974 , 0.03910262],\n",
       "       [0.01057306, 0.989427  ],\n",
       "       [0.03624627, 0.96375376],\n",
       "       [0.04614003, 0.95386   ],\n",
       "       [0.960307  , 0.039693  ],\n",
       "       [0.9584293 , 0.04157079],\n",
       "       [0.9602563 , 0.03974369],\n",
       "       [0.12021804, 0.8797819 ],\n",
       "       [0.43104073, 0.5689593 ],\n",
       "       [0.00727282, 0.99272716],\n",
       "       [0.06590642, 0.9340936 ],\n",
       "       [0.8266782 , 0.17332174],\n",
       "       [0.5225767 , 0.4774233 ],\n",
       "       [0.96040636, 0.03959357],\n",
       "       [0.15234801, 0.84765196],\n",
       "       [0.9538423 , 0.04615769],\n",
       "       [0.73307973, 0.26692027],\n",
       "       [0.04090774, 0.9590922 ],\n",
       "       [0.9603402 , 0.03965983],\n",
       "       [0.00824   , 0.9917601 ],\n",
       "       [0.9604726 , 0.03952743],\n",
       "       [0.23191269, 0.7680873 ],\n",
       "       [0.01355766, 0.9864423 ],\n",
       "       [0.02841082, 0.9715892 ],\n",
       "       [0.85147953, 0.1485204 ]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, unlike with training and validation sets, we do not pass the labels of the test set to the model during the inference stage.\n",
    "\n",
    "Each element in the predictions list is itself a list of length 2. The sum of the two values in each list is 1. The reason for this is because the two columns contain probabilities for each possible output: experienced side effects and did not experience side effects. Each element in the predictions list is a probability distribution over all possible outputs.\n",
    "\n",
    "The first column contains the probability for each patient not experiencing side effects, which is represented by a 0. The second column contains the probability for each patient experiencing side effects, which is represented by a 1.\n",
    "\n",
    "We can also look only at the most probable prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 0], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rounded_predictions = np.argmax(predictions, axis=-1)\n",
    "rounded_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create A Confusion Matrix For Neural Network Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true = test_labels, y_pred = rounded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[196  14]\n",
      " [ 10 200]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_plot_labels = ['no_side_effects','had_side_effects']\n",
    "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving And Loading The Model In Its Entirety\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 16)                32        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 642\n",
      "Trainable params: 642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving And Loading The Model In Its Entirety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This method of saving will save everything about the model – the architecture,\n",
    "#the weights, the optimizer, the state of the optimizer, the learning rate, the loss, etc.\n",
    "model.save('models/medical_trail.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 16)                32        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 642\n",
      "Trainable params: 642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "new_model = load_model('models/medical_trail.h5')\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving And Loading Only The Architecture Of The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 1], \"dtype\": \"float32\", \"sparse\": false, \"ragged\": false, \"name\": \"dense_input\"}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"batch_input_shape\": [null, 1], \"dtype\": \"float32\", \"units\": 16, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 32, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_2\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 2, \"activation\": \"softmax\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}]}, \"keras_version\": \"2.10.0\", \"backend\": \"tensorflow\"}'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There is another way we save only the architecture of the model. \n",
    "#This will not save the model weights, configurations, optimizer,\n",
    "#loss or anything else. This only saves the architecture of the model.\n",
    "\n",
    "#We can do this by calling model.to_json(). This will save the architecture\n",
    "#of the model as a JSON string. If we print out the string, we can see \n",
    "#exactly what this looks like.\n",
    "json_string = model.to_json()\n",
    "json_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 16)                32        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 642\n",
      "Trainable params: 642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "model_architecture = model_from_json(json_string)\n",
    "model_architecture.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving And Loading The Weights Of The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 16)                32        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 642\n",
      "Trainable params: 642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#We can do this by calling model.save_weights() and passing in \n",
    "#the path and file name to save the weights to with an h5 extension.\n",
    "model.save_weights('models/my_model_weights.h5')\n",
    "model.load_weights('models/my_model_weights.h5')\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "7baddf8d1f16f88c33226bd68d47327c021adaa5141d8df379be058b1fdf5dc8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
